# Prompt Debugging System Prompts

This repository is a small collection of system prompts designed for **debugging prompts** — whether system prompts, user prompts, or instructional prompts — across different use cases.

The collection spans both **conversational** and **instructional** contexts. It includes focused system prompts for areas such as tool/function calling, RAG (Retrieval-Augmented Generation) behavior, search result relevance, and structured output validation (e.g., JSON schemas).

While AI tools are traditionally seen as the *responders* to prompts, they can also serve as highly effective *analyzers* — applying their language understanding abilities to help users perform "self-examination" of prompts and identify why deviations occurred between expected and actual model outputs.  
Of course, human intelligence and strong prompt engineering skills remain essential, but AI can serve as a useful assistant in this process.

These system prompts can be configured as AI assistants on any platform that supports setting custom system prompts (such as OpenAI, OpenRouter, LM Studio, etc.).
 